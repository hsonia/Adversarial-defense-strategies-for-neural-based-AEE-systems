# -*- coding: utf-8 -*-
"""sentences_generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18ELHFLgGHuUx7082Rw4QAHNPTVdK0hYK
"""

import pickle
import torch
import joblib
from torch.utils.data import Dataset, DataLoader

from dataset_for_model import SentencesDataset
from sklearn.model_selection import train_test_split


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class DatasetGen(Dataset):
    def __init__(self, dataset):
        self.dataset = dataset
        
    def __getitem__(self, index):
        data = self.dataset.__getitem__(index)
        
        return data, index

    def __len__(self):
        return len(self.dataset)

class Generate_sent:
  def __init__(self, model, df_loader, corpus_dictionary, nbr_sents=None):

    self.model = model
    self.df_loader = df_loader
    self.corpus_dictionary = self._update_corpus_dict(corpus_dictionary)
    if nbr_sents!=None:
      self.nbr_sents = nbr_sents
    else:
      self.nbr_sents = len(self.df_loader)

  
  def _update_corpus_dict(self, corpus_dictionary):

    dict_wrd = {v: k for k, v in corpus_dictionary.items()}

    return dict_wrd

  def __call__(self):
    sents = []
    y = []
    for i, sample in enumerate(self.df_loader):
      (inp, _, attn_mask, mask), idx = sample
      
      pred_wrd = self.model(inp, mask, attn_mask)
      
      msk = mask.unsqueeze(-1).expand_as(pred_wrd)
      pred_wrd = pred_wrd.masked_fill(msk, 0)
      
      pred_word = inp.masked_fill(~mask, 0) + pred_wrd.argmax(-1)

      sent = [' '.join(self.corpus_dictionary[int(i)] for i in pred if int(i) not in [0,1,2,3,23784]) for pred in pred_word]
      
      y.append((len(sents), idx.item()))
      sents.append(sent)

      if i+1 == self.nbr_sents:
        return sents, y

if __name__ == '__main__':

  with open('corpus_dict_glv.pkl', 'rb') as f:
      corpus_dictionary = pickle.load(f)
      
  with open('clean_sents.pkl', 'rb') as f:
      clean_sents = pickle.load(f)
      
  from model_generator import Generator
  # in case we don't have saved joblib model
  dataset = SentencesDataset(clean_sents, corpus_dictionary)

  embedding_dim = maxLen = (dataset.seqLen)
  vocab_size = len(dataset.corpus_dictionary)+1 #vocab_size = 23793
  dropout = 0.5
  nbr_layers = nbr_heads = 2
  
  try:
    model = Generator(vocab_size, maxLen, embedding_dim, dropout, nbr_layers, nbr_heads).to(device)
    model.load_state_dict(torch.load('generator_17_1669891013.67.pt')['model_state'])
  except:
    model = Generator(vocab_size, maxLen, embedding_dim, dropout, nbr_layers, nbr_heads).to(device)

  _, dataset_test = train_test_split(dataset, test_size=0.2, random_state = 41)
  dataset_test = DatasetGen(dataset_test)
  df_loader = DataLoader(dataset_test, batch_size=1, shuffle=True, num_workers=1)


    
  
  # gen = Generate_sent(model, df_loader, corpus_dictionary, nbr_sents=20)
  gen = Generate_sent(model, df_loader, corpus_dictionary)
  
  sents, y_to_gen_nn = gen()
  
  import itertools
  sents_nn = list(itertools.chain.from_iterable(sents))