# -*- coding: utf-8 -*-
"""main_generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/115JmqThkRHuWXmVM4HKSPjaVn_suZn3y
"""

from generation import GEN_SAMPLES, GEN_VECTORS, AUGSAM
from sentences_generator import DatasetGen, Generate_sent

import subprocess
import sys

subprocess.check_call([sys.executable, "-m", "pip", "install", 'nlpaug'])
import nlpaug.augmenter.word as naw
import random
import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

from sklearn.metrics import cohen_kappa_score, pairwise
import more_itertools

import numpy as np
import pandas as pd


import pickle
import torch
from torch.utils.data import Dataset, DataLoader

from dataset_for_model import SentencesDataset
from sklearn.model_selection import train_test_split

"""#new samples generation"""

from data import Data
from preprocess_data import Preprocess, VectorRepresentationModels
import itertools

"""### gen samples using vectorization models"""

data = Data()
preprocess = Preprocess(data)
y = preprocess.y
vrm = VectorRepresentationModels(preprocess)

#create models and dicts
glv_model, glv_Dict, corpus = vrm.glove(epochs=500)
w2v_model, w2v_Dict = vrm.word2vec()
tfidf_vectorizer, bows = vrm.tf_idf()

#create vectors for ML models
clean_sents = preprocess()
glv_vec = vrm.cleanSent_vec(glv_model, glv_Dict)
w2v_vec = vrm.cleanSent_vec(w2v_model, w2v_Dict)

glv_syn = {}
for word in glv_model.dictionary:
  glv_syn[word] = glv_model.most_similar(word, 2)[0][0]
  
glv_ant = {}
for word in glv_model.dictionary:
  glv_ant[word] = glv_model.most_similar(word, -1)[-1][0]

w2v_syn = {}
for word in w2v_Dict.keys():
  w2v_syn[word] = w2v_model.wv.most_similar(word)[0][0]

w2v_ant = {}
for word in w2v_Dict.keys():
  w2v_ant[word] = w2v_model.wv.most_similar(negative=[word])[0][0]

X_train_glv, X_test_glv, y_train_glv, y_test_glv = train_test_split(glv_vec, y, test_size=0.25, random_state=41)

gen_samples_glv = GEN_SAMPLES(vrm, glv_model, glv_Dict, glv_syn, glv_ant)

new_samples_glv, new_grad_glv = gen_samples_glv.generate_samples()

X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(w2v_vec, y, test_size=0.25, random_state=41)

gen_samples_w2v = GEN_SAMPLES(vrm, w2v_model, w2v_Dict, w2v_syn, w2v_ant)

new_samples_w2v, new_grad_w2v = gen_samples_w2v.generate_samples()

"""### gen samples using NN model"""

with open('corpus_dict_glv.pkl', 'rb') as f:
    corpus_dictionary = pickle.load(f)
    
with open('clean_sents.pkl', 'rb') as f:
    clean_sents = pickle.load(f)
    
from model_generator import Generator

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# in case we don't have saved joblib model
dataset = SentencesDataset(clean_sents, corpus_dictionary)

embedding_dim = maxLen = (dataset.seqLen)
vocab_size = len(dataset.corpus_dictionary)+1 #vocab_size = 23793
dropout = 0.5
nbr_layers = nbr_heads = 2

try:
  model = Generator(vocab_size, maxLen, embedding_dim, dropout, nbr_layers, nbr_heads).to(device)
  model.load_state_dict(torch.load('generator_17_1669891013.67.pt')['model_state'])
except:
  model = Generator(vocab_size, maxLen, embedding_dim, dropout, nbr_layers, nbr_heads).to(device)

_, dataset_test = train_test_split(dataset, test_size=0.2, random_state = 41)
dataset_test = DatasetGen(dataset_test)
df_loader = DataLoader(dataset_test, batch_size=1, shuffle=True, num_workers=1)


  

# gen = Generate_sent(model, df_loader, corpus_dictionary, nbr_sents=20)
gen = Generate_sent(model, df_loader, corpus_dictionary)

sents, y_to_gen_nn = gen()

import itertools
sents_nn = list(itertools.chain.from_iterable(sents))

"""#generate vectors"""

#generate vector using glove model

gen_vec_glv = GEN_VECTORS(new_samples_glv, new_grad_glv, vrm, glv_model, glv_Dict)
new_x_train_glv = gen_vec_glv.generate_vectors(y_train_glv)
new_y_train_glv = gen_vec_glv.generate_grades(new_x_train_glv, X_train_glv, y_train_glv)

#generate vector using w2v model

gen_vec_w2v = GEN_VECTORS(new_samples_w2v, new_grad_w2v, vrm, w2v_model, w2v_Dict)
new_x_train_w2v = gen_vec_w2v.generate_vectors(y_train_w2v)
new_y_train_w2v = gen_vec_w2v.generate_grades(new_x_train_w2v, X_train_w2v, y_train_w2v)

#generate vector using nn model

gen_vec_nn = GEN_VECTORS(sents_nn, y_to_gen_nn, vrm, glv_model, glv_Dict, True)
new_vec_nn = gen_vec_nn.generate_vectors(None)
new_y_nn = gen_vec_nn.generate_grades(new_vec_nn, X_test_glv, y_test_glv)

"""#augmente data"""

aug_vec_glv, aug_y_glv = AUGSAM(X_train_glv, y_train_glv, new_x_train_glv, new_y_train_glv)()
aug_vec_w2v, aug_y_w2v = AUGSAM(X_train_w2v, y_train_w2v, new_x_train_w2v, new_y_train_w2v)()
aug_vec_nn, aug_y_nn = AUGSAM(X_train_glv, y_train_glv, new_vec_nn, new_y_nn)()

"""#save data"""

random_state = 41
test_size = 0.25
file = open('org_glv_w2v.pkl','wb')
pickle.dump(glv_vec, file)
pickle.dump(w2v_vec, file)
pickle.dump(y, file)
pickle.dump(random_state, file)
pickle.dump(test_size, file)
file.close()

file = open('org_gen_aug_glv.pkl','wb')
# pickle.dump(X_train_glv, file)
# pickle.dump(y_train_glv, file)
pickle.dump(new_x_train_glv, file)
pickle.dump(new_y_train_glv, file)
pickle.dump(aug_vec_glv, file)
pickle.dump(aug_y_glv, file)
file.close()

file = open('org_gen_aug_w2v.pkl','wb')
# pickle.dump(X_train_w2v, file)
# pickle.dump(y_train_w2v, file)
pickle.dump(new_x_train_w2v, file)
pickle.dump(new_y_train_w2v, file)
pickle.dump(aug_vec_w2v, file)
pickle.dump(aug_y_w2v, file)
file.close()

file = open('org_gen_aug_nn.pkl','wb')
# pickle.dump(X_train_glv, file)
# pickle.dump(y_train_glv, file)
pickle.dump(new_vec_nn, file)
pickle.dump(new_y_nn, file)
pickle.dump(aug_vec_nn, file)
pickle.dump(aug_y_nn, file)
file.close()

